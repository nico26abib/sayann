#!/usr/bin/env python3
"""
Test complet: Chercher le S&P 500 sur Bloomberg via Google
Ce test simule exactement ce que le bot doit faire
"""
import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import config
import re

async def test_sp500_search():
    print("üß™ Test: Chercher S&P 500 actuel sur Bloomberg\n")
    
    cdp_url = f"http://localhost:{config.CHROME_CDP_PORT}"
    print(f"üîó Connexion √† Chrome: {cdp_url}")
    
    playwright = await async_playwright().start()
    
    try:
        # Connexion au Chrome existant
        browser = await playwright.chromium.connect_over_cdp(cdp_url)
        print("‚úÖ Connect√© √† ton Chrome!\n")
        
        # Utilise le premier contexte ou en cr√©e un
        if browser.contexts:
            context = browser.contexts[0]
        else:
            context = await browser.new_context()
        
        page = await context.new_page()
        
        # √âtape 1: Recherche Google
        query = "S&P 500 Bloomberg"
        search_url = f"https://www.google.com/search?q={query.replace(' ', '+')}"
        
        print(f"üìù √âtape 1: Recherche Google")
        print(f"   Query: {query}")
        print(f"   URL: {search_url}\n")
        
        await page.goto(search_url, timeout=15000)
        await page.wait_for_load_state("domcontentloaded")
        await asyncio.sleep(2)  # Laisse le temps √† la page de charger
        
        print("‚úÖ Page Google charg√©e\n")
        
        # Screenshot de Google
        await page.screenshot(path="test_sp500_google.png")
        print("üì∏ Screenshot sauvegard√©: test_sp500_google.png\n")
        
        # √âtape 2: Extraction des r√©sultats
        print(f"üìä √âtape 2: Extraction des donn√©es")
        
        content = await page.content()
        soup = BeautifulSoup(content, 'html.parser')
        
        # Strat√©gie 1: Chercher les featured snippets / knowledge panel
        print("\nüîç Strat√©gie 1: Featured snippets / Knowledge panel")
        featured = soup.find_all(['div', 'span'], class_=re.compile('(kCrYT|Z0LcW|IZ6rdc|kp-header)'))
        for f in featured[:5]:
            text = f.get_text(strip=True)
            if len(text) > 10 and len(text) < 200:
                print(f"   ‚Üí {text}")
        
        # Strat√©gie 2: Chercher tous les chiffres qui ressemblent √† un index
        print("\nüîç Strat√©gie 2: Recherche de chiffres (format index)")
        all_text = soup.get_text()
        
        # Patterns pour S&P 500: 5,873.42 ou 5873.42 ou 5,873
        patterns = [
            r'S&P[^\d]*?(\d{1,2}[,.]?\d{3}\.?\d{0,2})',  # S&P 500: 5,873.42
            r'(\d{1,2}[,.]?\d{3}\.?\d{0,2}).*?(?:S&P|points?)',  # 5,873.42 ... S&P
        ]
        
        found_values = set()
        for pattern in patterns:
            matches = re.findall(pattern, all_text, re.IGNORECASE)
            for match in matches:
                clean = match.replace(',', '').replace(' ', '')
                try:
                    val = float(clean)
                    # S&P 500 est g√©n√©ralement entre 1000 et 10000
                    if 1000 <= val <= 10000:
                        found_values.add(clean)
                except:
                    pass
        
        if found_values:
            print(f"   Valeurs trouv√©es: {sorted(found_values, reverse=True)}")
        
        # Strat√©gie 3: Chercher dans les h3 (titres des r√©sultats)
        print("\nüîç Strat√©gie 3: Titres des r√©sultats Google")
        h3s = soup.find_all('h3')
        for h3 in h3s[:5]:
            text = h3.get_text()
            print(f"   ‚Üí {text}")
        
        # Strat√©gie 4: Chercher des divs avec des classes sp√©cifiques
        print("\nüîç Strat√©gie 4: Divs de r√©sultats")
        result_divs = soup.find_all('div', class_=['g', 'N54PNb', 'BNeawe'])
        for div in result_divs[:5]:
            text = div.get_text(strip=True)
            if 'S&P' in text or '500' in text:
                # Extrait juste une partie
                snippet = text[:150] + "..." if len(text) > 150 else text
                print(f"   ‚Üí {snippet}")
        
        # Strat√©gie 5: Cliquer sur le premier lien Bloomberg si trouv√©
        print("\nüîç Strat√©gie 5: Cherche lien Bloomberg")
        links = await page.query_selector_all('a[href*="bloomberg"]')
        
        if links:
            print(f"   ‚úÖ {len(links)} lien(s) Bloomberg trouv√©(s)")
            
            # Clique sur le premier
            try:
                first_link = links[0]
                href = await first_link.get_attribute('href')
                print(f"   üîó Clic sur: {href[:100]}...")
                
                await first_link.click()
                await page.wait_for_load_state("domcontentloaded")
                await asyncio.sleep(2)
                
                print("   ‚úÖ Page Bloomberg charg√©e\n")
                
                # Screenshot de Bloomberg
                await page.screenshot(path="test_sp500_bloomberg.png")
                print("üì∏ Screenshot Bloomberg: test_sp500_bloomberg.png\n")
                
                # Extrait le contenu de Bloomberg
                bloomberg_content = await page.content()
                bloomberg_soup = BeautifulSoup(bloomberg_content, 'html.parser')
                
                # Cherche le prix sur Bloomberg
                print("üí∞ Extraction du prix sur Bloomberg:")
                
                # Cherche dans le titre
                title = await page.title()
                print(f"   Title: {title}")
                
                # Cherche des spans/divs avec des chiffres
                price_elements = bloomberg_soup.find_all(['span', 'div'], class_=re.compile('(price|value|quote)'))
                for elem in price_elements[:10]:
                    text = elem.get_text(strip=True)
                    if re.search(r'\d{1,2}[,.]?\d{3}', text):
                        print(f"   ‚Üí {text}")
                
                # Cherche tous les gros chiffres
                all_bloomberg_text = bloomberg_soup.get_text()
                sp500_values = re.findall(r'(\d{1,2}[,.]?\d{3}\.?\d{0,2})', all_bloomberg_text)
                sp500_values = [v for v in sp500_values if 1000 <= float(v.replace(',', '')) <= 10000]
                
                if sp500_values:
                    print(f"\n   üìä Valeurs potentielles du S&P 500:")
                    for val in sorted(set(sp500_values), reverse=True)[:5]:
                        print(f"      ‚Ä¢ {val}")
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Erreur clic Bloomberg: {e}")
        else:
            print("   ‚ö†Ô∏è  Aucun lien Bloomberg trouv√© dans les r√©sultats")
        
        # R√©sum√© final
        print("\n" + "="*60)
        print("üìà R√âSULTAT FINAL")
        print("="*60)
        
        if found_values:
            best_value = sorted(found_values, reverse=True)[0]
            print(f"‚úÖ S&P 500 estim√©: {best_value}")
            print(f"   (trouv√© sur la page Google)")
        else:
            print("‚ö†Ô∏è  Valeur du S&P 500 non trouv√©e automatiquement")
            print("   Regarde les screenshots pour analyse manuelle:")
            print("   - test_sp500_google.png")
            print("   - test_sp500_bloomberg.png")
        
        print("\n‚è∏  Fen√™tre reste ouverte 10 secondes...")
        await asyncio.sleep(10)
        
        await page.close()
        await browser.close()
        
    except Exception as e:
        print(f"\n‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        
        print(f"\nüí° Solution:")
        print(f"   1. Lance Chrome: ./scripts/start_chrome_debug.sh")
        print(f"   2. V√©rifie: curl http://localhost:9222/json/version")
        print(f"   3. Relance ce test")
        
    finally:
        await playwright.stop()
        print("\n‚úÖ Test termin√©")

if __name__ == "__main__":
    asyncio.run(test_sp500_search())

